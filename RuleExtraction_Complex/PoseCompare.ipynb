{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/open-mmlab/mmcv.git\n",
    "!git clone https://github.com/ViTAE-Transformer/ViTPose.git\n",
    "!git clone https://github.com/shubham-goel/4D-Humans.git FDHumans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make conda environment: Run the following lines in a terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda activate PoseCompare\n",
    "cd mmcv\n",
    "git checkout v1.3.9\n",
    "# For Unix:\n",
    "MMCV_WITH_OPS=1 pip install -e .\n",
    "# For windows CMD LINE:\n",
    "set MMCV_WITH_OPS=1 && pip install -e .\n",
    "\n",
    "# if you experience error with mmcv and mmcv-full install: try the following\n",
    "pip uninstall mmcv mmcv-full\n",
    "pip install mmcv-full==1.4.8 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\n",
    "\n",
    "cd ..\n",
    "git clone https://github.com/ViTAE-Transformer/ViTPose.git\n",
    "cd ViTPose\n",
    "pip install -v -e .\n",
    "\n",
    "mkdir checkpoints\n",
    "cd ./checkpoints\n",
    "curl.exe -L -o vitpose_checkpoint.pth \"https://62afda.dm.files.1drv.com/y4mBDiqHvl4ClkQbjljDfxZ35JemNwe-D-YlTuMfeya1BIR5tVP3cO26ntjrJkBL-2L8beSmOOPy7149gWRMkDqTZCPhS--XxryYZLSGtdKxR5ADq-9S_6ApoHxLbQP4MOs63iPz2jSLQMFqJFcFdoXZ2ml2HyvGkCu7MxyP9ELoZvtYRyipBDvsFvR2bN7xUknS6LR5HdBjGpZtM7saMmIXQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd 4DHumans\n",
    "pip install -e .[all]\n",
    "# if detectron2 failed to install, try install it directly:\n",
    "pip install git+https://github.com/facebookresearch/detectron2\n",
    "\n",
    "pip install git+https://github.com/brjathu/PHALP.git\n",
    "\n",
    "wget https://github.com/classner/up/raw/821a390fbf87a522fb327fc46736eda0326e2a06/models/3D/basicModel_neutral_lbs_10_207_0_v1.0.0.pkl\n",
    "mkdir data/\n",
    "cp basicModel_neutral_lbs_10_207_0_v1.0.0.pkl ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import mediapipe as mp\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "from mmcv import Config\n",
    "from ViTPose.mmpose.models import build_posenet\n",
    "from mmcv.mmcv.runner import load_checkpoint\n",
    "\n",
    "# Increase recursion limit\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitPoseWrapper:\n",
    "    \"\"\"Wrapper for ViTPose model.\"\"\"\n",
    "    def __init__(self, config_path, weights_path, input_size=(192, 256), device=None):\n",
    "        self.input_width, self.input_height = input_size\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        try:\n",
    "            # Load configuration and model\n",
    "            cfg = Config.fromfile(config_path)\n",
    "            self.model = build_posenet(cfg.model)\n",
    "            load_checkpoint(self.model, weights_path, map_location=self.device)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "            # Transformation pipeline\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ])\n",
    "\n",
    "            # Keypoint connections (COCO format)\n",
    "            self.keypoint_connections = [\n",
    "                (15, 13), (13, 11), (16, 14), (14, 12),  # Limbs\n",
    "                (11, 12), (5, 11), (6, 12), (5, 6),      # Hips to shoulders\n",
    "                (5, 7), (6, 8), (7, 9), (8, 10),        # Neck to arms\n",
    "                (1, 2), (0, 1), (0, 2), (1, 3), (2, 4), (3, 5), (4, 6)  # Face and shoulders\n",
    "            ]\n",
    "\n",
    "            logging.info(\"ViTPose model initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize ViTPose model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Preprocess frame for model input.\"\"\"\n",
    "        try:\n",
    "            original_height, original_width = frame.shape[:2]\n",
    "            \n",
    "            # Simple resize to model input size\n",
    "            frame_resized = cv2.resize(frame, (self.input_width, self.input_height))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = self.transform(frame_rgb).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Store metadata for rescaling predictions back to original size\n",
    "            img_metas = [{\n",
    "                'img_shape': (self.input_height, self.input_width, 3),\n",
    "                'original_shape': (original_height, original_width, 3),\n",
    "                'scale_factor': np.array([\n",
    "                    original_width / self.input_width,\n",
    "                    original_height / self.input_height\n",
    "                ]),\n",
    "                'center': np.array([self.input_width // 2, self.input_height // 2]),\n",
    "                'scale': np.array([1.0, 1.0]),\n",
    "                'rotation': 0,\n",
    "                'flip_pairs': None,\n",
    "                'dataset_idx': 0,\n",
    "                'image_file': None\n",
    "            }]\n",
    "            return input_tensor, img_metas\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preprocessing frame: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Run inference on a single frame and return keypoints.\"\"\"\n",
    "        try:\n",
    "            input_tensor, img_metas = self.preprocess_frame(frame)\n",
    "            if input_tensor is None:\n",
    "                return None\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model(img=input_tensor, img_metas=img_metas, return_loss=False)\n",
    "                return output['preds'][0]  # Extract keypoints for the first person\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing frame with ViTPose: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from FDHumans.hmr2.models import load_hmr2, DEFAULT_CHECKPOINT\n",
    "from FDHumans.hmr2.datasets.utils import recursive_to\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourDHumanWrapper:\n",
    "    \"\"\"Wrapper for 4DHuman model.\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path=None, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the 4DHuman model.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_path (str): Path to the model checkpoint.\n",
    "            device (torch.device): Device to load the model on (CPU or GPU).\n",
    "        \"\"\"\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.checkpoint_path = checkpoint_path or DEFAULT_CHECKPOINT\n",
    "\n",
    "        try:\n",
    "            # Load the model and move to device\n",
    "            self.model, self.model_cfg = load_hmr2(self.checkpoint_path)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "            logging.info(\"4DHuman model initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize 4DHuman model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Preprocess a video frame for the 4DHuman model.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): Input frame in BGR format.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Preprocessed frame as a tensor.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            frame_resized = cv2.resize(frame, (224, 224))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB) / 255.0\n",
    "            input_tensor = torch.tensor(frame_rgb.transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0)\n",
    "            return input_tensor.to(self.device)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preprocessing frame: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Run inference on a single frame and return the human mesh and other predictions.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): Input frame in BGR format.\n",
    "\n",
    "        Returns:\n",
    "            dict: Model output, including vertices and camera parameters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Preprocess the frame\n",
    "            input_tensor = self.preprocess_frame(frame)\n",
    "            if input_tensor is None:\n",
    "                return None\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                batch = {\"img\": input_tensor}\n",
    "                batch = recursive_to(batch, self.device)\n",
    "                output = self.model(batch)\n",
    "            \n",
    "            return output\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing frame with 4DHuman model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def render_mesh(self, frame, vertices, camera_params):\n",
    "        \"\"\"\n",
    "        Render the human mesh on the input frame.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): Original frame (BGR format).\n",
    "            vertices (np.ndarray): 3D vertices of the mesh (6890, 3).\n",
    "            camera_params (np.ndarray): Camera parameters for projection.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Frame with rendered mesh.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s, tx, ty = camera_params  # Scale and translation\n",
    "            img_h, img_w = frame.shape[:2]\n",
    "            projected_vertices = vertices[:, :2] * s + np.array([tx, ty])\n",
    "            projected_vertices[:, 0] = (projected_vertices[:, 0] + 1) * img_w / 2.0\n",
    "            projected_vertices[:, 1] = (1 - projected_vertices[:, 1]) * img_h / 2.0\n",
    "\n",
    "            # Draw the vertices on the frame\n",
    "            mesh_frame = frame.copy()\n",
    "            for v in projected_vertices.astype(int):\n",
    "                cv2.circle(mesh_frame, tuple(v), 2, (0, 255, 0), -1)  # Green dots for vertices\n",
    "            \n",
    "            return mesh_frame\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error rendering mesh: {e}\")\n",
    "            return frame\n",
    "        \n",
    "    def get_display_frame(self, frame):\n",
    "        \"\"\"Process the frame and return the rendered mesh frame.\"\"\"\n",
    "        try:\n",
    "            output = self.process_frame(frame)\n",
    "            if output is not None:\n",
    "                vertices = output[\"pred_vertices\"][0].cpu().numpy()\n",
    "                camera_params = output[\"pred_cam\"][0].cpu().numpy()\n",
    "                return self.render_mesh(frame, vertices, camera_params)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating display frame: {e}\")\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEstimationUI:\n",
    "    def __init__(self, root,vitpose_weights_path = None, fourDHuman_weight_path = None, vitpose_config_path = None):\n",
    "        \n",
    "        self.root = root\n",
    "        self.root.title(\"Pose Estimation Comparison\")\n",
    "        self.vitpose_weights_path = vitpose_weights_path\n",
    "        self.vitpose_config_path = vitpose_config_path\n",
    "        self.fourDHuman_weight_path = fourDHuman_weight_path\n",
    "        # Video state\n",
    "        self.video_path = None\n",
    "        self.cap = None\n",
    "        self.playing = False\n",
    "        self.current_frame = 0\n",
    "        self.total_frames = 0\n",
    "        self.frame_cache = {}  # Cache for loaded frames\n",
    "        \n",
    "        # Initialize models\n",
    "        self.setup_models()\n",
    "        \n",
    "        # Create UI elements\n",
    "        self.create_ui()\n",
    "        \n",
    "        # Update timer\n",
    "        self.update_interval = 30  # milliseconds\n",
    "        self.update_id = None\n",
    "\n",
    "        self.view_states = {\n",
    "            'mediapipe': {'show_background': True},\n",
    "            'vitpose': {'show_background': True},\n",
    "            '4dhuman': {'show_background': True}\n",
    "        }\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize pose estimation models\"\"\"\n",
    "        try:\n",
    "            self.FDHmodel = FourDHumanWrapper()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize 4DHumans: {e}\")\n",
    "            self.FDHmodel = None\n",
    "        try:\n",
    "            self.mp_pose = mp.solutions.pose\n",
    "            self.mp_drawing = mp.solutions.drawing_utils\n",
    "            self.mediapipe_pose = self.mp_pose.Pose(\n",
    "                static_image_mode=False,\n",
    "                model_complexity=2,\n",
    "                min_detection_confidence=0.5\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize MediaPipe: {e}\")\n",
    "            self.mediapipe_pose = None\n",
    "        # Load the model\n",
    "        # cfg = Config.fromfile('ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/ochu/vitpose_base_coco_256x192.py')\n",
    "        # cfg = Config.fromfile('ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_small_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py')\n",
    "        # cfg = Config.fromfile('ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_base_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py')\n",
    "        # cfg = Config.fromfile('ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_large_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py')\n",
    "\n",
    "        # self.vitpose = build_posenet(cfg.model)\n",
    "        # load_checkpoint(model, './ViTPose/weights/vitpose_large_coco_aic_mpii.pth', map_location='cpu')\n",
    "        # load_checkpoint(model, './ViTPose/weights/vitpose+_small.pth', map_location='cpu')\n",
    "        # load_checkpoint(self.vitpose,'./ViTPose/weights/vitpose+_base.pth',map_location='cpu')\n",
    "        # try:\n",
    "        #     load_checkpoint(model,'./ViTPose/weights/vitpose+_large.pth',map_location='cuda')\n",
    "        # except Exception as e:\n",
    "        #     load_checkpoint(model,'./ViTPose/weights/vitpose+_large.pth',map_location='cpu')\n",
    "\n",
    "        # if self.vitpose_weights_path:  # You'll need to define this path\n",
    "        #     try:\n",
    "        #         self.vitpose = VitPoseWrapper(self.vitpose_weights_path)\n",
    "        #         logging.info(\"VitPose model initialized successfully\")\n",
    "        #     except Exception as e:\n",
    "        #         logging.error(f\"Failed to initialize VitPose: {e}\")\n",
    "        try:\n",
    "            # Initialize ViTPose\n",
    "            config_path = self.vitpose_config_path\n",
    "            weights_path = self.vitpose_weights_path\n",
    "            self.vitpose_wrapper = VitPoseWrapper(config_path, weights_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize ViTPose: {e}\")\n",
    "            self.vitpose_wrapper = None\n",
    "    def create_ui(self):\n",
    "        \"\"\"Create the user interface\"\"\"\n",
    "        # Main container\n",
    "        main_container = ttk.Frame(self.root)\n",
    "        main_container.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Top control panel\n",
    "        control_frame = ttk.Frame(main_container)\n",
    "        control_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "\n",
    "        # Add view toggle panel\n",
    "        toggle_frame = ttk.Frame(main_container)\n",
    "        toggle_frame.pack(fill=tk.X, padx=5, pady=2)\n",
    "        \n",
    "        # View toggle buttons\n",
    "        ttk.Label(toggle_frame, text=\"View Options:\").pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        def create_toggle_button(model_name):\n",
    "            var = tk.BooleanVar(value=True)\n",
    "            btn = ttk.Checkbutton(\n",
    "                toggle_frame,\n",
    "                text=f\"Show {model_name} Background\",\n",
    "                variable=var,\n",
    "                command=lambda: self.toggle_view(model_name.lower(), var.get())\n",
    "            )\n",
    "            btn.pack(side=tk.LEFT, padx=5)\n",
    "            return var\n",
    "            \n",
    "        self.toggle_vars = {\n",
    "            'mediapipe': create_toggle_button('MediaPipe'),\n",
    "            'vitpose': create_toggle_button('VitPose'),\n",
    "            '4DHuman': create_toggle_button('4DHuman')\n",
    "        }\n",
    "        \n",
    "        # Video display area\n",
    "        display_frame = ttk.Frame(main_container)\n",
    "        display_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "        # Video selection\n",
    "        self.file_button = ttk.Button(control_frame, text=\"Open Video\", command=self.safe_open_video)\n",
    "        self.file_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Playback controls\n",
    "        self.prev_button = ttk.Button(control_frame, text=\"⏮\", command=self.prev_frame)\n",
    "        self.prev_button.pack(side=tk.LEFT, padx=2)\n",
    "        \n",
    "        self.play_button = ttk.Button(control_frame, text=\"▶\", command=self.toggle_play)\n",
    "        self.play_button.pack(side=tk.LEFT, padx=2)\n",
    "        \n",
    "        self.next_button = ttk.Button(control_frame, text=\"⏭\", command=self.next_frame)\n",
    "        self.next_button.pack(side=tk.LEFT, padx=2)\n",
    "        \n",
    "        # Frame slider\n",
    "        self.frame_slider = ttk.Scale(control_frame, from_=0, to=100, orient=tk.HORIZONTAL)\n",
    "        self.frame_slider.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)\n",
    "        self.frame_slider.bind(\"<ButtonRelease-1>\", self.on_slider_release)\n",
    "        \n",
    "        # Frame counter\n",
    "        self.frame_label = ttk.Label(control_frame, text=\"Frame: 0/0\")\n",
    "        self.frame_label.pack(side=tk.RIGHT, padx=5)\n",
    "        \n",
    "        # Video display area\n",
    "        display_frame = ttk.Frame(main_container)\n",
    "        display_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Configure grid\n",
    "        display_frame.grid_columnconfigure((0,1,2,3), weight=1)\n",
    "        display_frame.grid_rowconfigure(0, weight=1)\n",
    "        \n",
    "        # Create canvases for video display\n",
    "        canvas_width = 320\n",
    "        canvas_height = 240\n",
    "        \n",
    "        # Original video\n",
    "        self.original_canvas = tk.Canvas(display_frame, width=canvas_width, height=canvas_height, bg='black')\n",
    "        self.original_canvas.grid(row=0, column=0, padx=5, pady=5, sticky='nsew')\n",
    "        ttk.Label(display_frame, text=\"Original\").grid(row=1, column=0)\n",
    "        \n",
    "        # MediaPipe output\n",
    "        self.mediapipe_canvas = tk.Canvas(display_frame, width=canvas_width, height=canvas_height, bg='black')\n",
    "        self.mediapipe_canvas.grid(row=0, column=1, padx=5, pady=5, sticky='nsew')\n",
    "        ttk.Label(display_frame, text=\"MediaPipe\").grid(row=1, column=1)\n",
    "        \n",
    "        # VitPose output\n",
    "        self.vitpose_canvas = tk.Canvas(display_frame, width=canvas_width, height=canvas_height, bg='black')\n",
    "        self.vitpose_canvas.grid(row=0, column=2, padx=5, pady=5, sticky='nsew')\n",
    "        ttk.Label(display_frame, text=\"VitPose\").grid(row=1, column=2)\n",
    "\n",
    "        # 4DHuman output\n",
    "        self.fourdhuman_canvas = tk.Canvas(display_frame, width=canvas_width, height=canvas_height, bg='black')\n",
    "        self.fourdhuman_canvas.grid(row=0, column=4, padx=5, pady=5, sticky='nsew')\n",
    "        ttk.Label(display_frame, text=\"4DHuman\").grid(row=1, column=4)\n",
    "\n",
    "    def toggle_view(self, model_name, show_background):\n",
    "        \"\"\"Toggle between full view and skeleton/mesh only view\"\"\"\n",
    "        self.view_states[model_name]['show_background'] = show_background\n",
    "        self.update_frame()\n",
    "\n",
    "    def safe_open_video(self):\n",
    "        \"\"\"Safely open video file dialog\"\"\"\n",
    "        try:\n",
    "            filename = filedialog.askopenfilename(\n",
    "                filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
    "            )\n",
    "            if filename:\n",
    "                self.load_video(filename)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error opening video: {e}\")\n",
    "            tk.messagebox.showerror(\"Error\", f\"Failed to open video: {str(e)}\")\n",
    "            \n",
    "    def load_video(self, filename):\n",
    "        \"\"\"Load the selected video file\"\"\"\n",
    "        try:\n",
    "            if self.cap is not None:\n",
    "                self.cap.release()\n",
    "                self.cap = None\n",
    "            \n",
    "            self.cap = cv2.VideoCapture(filename)\n",
    "            if not self.cap.isOpened():\n",
    "                raise ValueError(\"Failed to open video file\")\n",
    "                \n",
    "            self.video_path = filename\n",
    "            self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            self.current_frame = 0\n",
    "            self.frame_cache.clear()\n",
    "            \n",
    "            # Update slider\n",
    "            self.frame_slider.configure(to=self.total_frames - 1)\n",
    "            self.frame_slider.set(0)\n",
    "            \n",
    "            # Update first frame\n",
    "            self.update_frame()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading video: {e}\")\n",
    "            tk.messagebox.showerror(\"Error\", f\"Failed to load video: {str(e)}\")\n",
    "            \n",
    "    def on_slider_release(self, event):\n",
    "        \"\"\"Handle slider release event\"\"\"\n",
    "        try:\n",
    "            self.current_frame = int(self.frame_slider.get())\n",
    "            self.update_frame()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating frame: {e}\")\n",
    "            \n",
    "    def toggle_play(self):\n",
    "        \"\"\"Toggle video playback\"\"\"\n",
    "        if self.cap is None:\n",
    "            return\n",
    "            \n",
    "        self.playing = not self.playing\n",
    "        self.play_button.configure(text=\"⏸\" if self.playing else \"▶\")\n",
    "        \n",
    "        if self.playing:\n",
    "            self.play()\n",
    "        elif self.update_id:\n",
    "            self.root.after_cancel(self.update_id)\n",
    "            self.update_id = None\n",
    "            \n",
    "    def safe_read_frame(self):\n",
    "        \"\"\"Safely read a frame from video\"\"\"\n",
    "        try:\n",
    "            if self.current_frame in self.frame_cache:\n",
    "                return True, self.frame_cache[self.current_frame]\n",
    "                \n",
    "            self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.current_frame)\n",
    "            ret, frame = self.cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                self.frame_cache[self.current_frame] = frame\n",
    "            \n",
    "            return ret, frame\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading frame: {e}\")\n",
    "            return False, None\n",
    "            \n",
    "    def update_frame(self):\n",
    "        \"\"\"Update frame display\"\"\"\n",
    "        if self.cap is None:\n",
    "            return\n",
    "            \n",
    "        ret, frame = self.safe_read_frame()\n",
    "        if not ret:\n",
    "            self.playing = False\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Update displays\n",
    "            self.update_original_display(frame)\n",
    "            self.update_mediapipe_display(frame.copy())\n",
    "            self.update_vitpose_display(frame.copy())\n",
    "            # self.update_wham_display(frame.copy())\n",
    "            self.update_fourdhuman_display(frame.copy())\n",
    "\n",
    "            # Update slider and label\n",
    "            self.frame_slider.set(self.current_frame)\n",
    "            self.frame_label.configure(text=f\"Frame: {self.current_frame}/{self.total_frames-1}\")\n",
    "            \n",
    "            # Increment frame counter if playing\n",
    "            if self.playing:\n",
    "                self.current_frame = (self.current_frame + 1) % self.total_frames\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating displays: {e}\")\n",
    "            \n",
    "    def prepare_photo(self, frame):\n",
    "        \"\"\"Safely convert frame to PhotoImage\"\"\"\n",
    "        try:\n",
    "            frame = cv2.resize(frame, (320, 240))\n",
    "            image = Image.fromarray(frame)\n",
    "            return ImageTk.PhotoImage(image)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error preparing photo: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def update_original_display(self, frame):\n",
    "        \"\"\"Update original video display\"\"\"\n",
    "        try:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            photo = self.prepare_photo(frame_rgb)\n",
    "            if photo:\n",
    "                self.original_canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "                self.original_canvas.photo = photo\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating original display: {e}\")\n",
    "\n",
    "    def update_vitpose_display(self, frame):\n",
    "        \"\"\"Update ViTPose output display with skeleton visualization.\"\"\"\n",
    "        try:\n",
    "            if self.vitpose_wrapper:\n",
    "                # Create display frame based on view state\n",
    "                if not self.view_states['vitpose']['show_background']:\n",
    "                    display_frame = np.zeros(frame.shape, dtype=np.uint8)\n",
    "                else:\n",
    "                    display_frame = frame.copy()\n",
    "\n",
    "                # Run ViTPose inference\n",
    "                keypoints = self.vitpose_wrapper.process_frame(frame)\n",
    "                \n",
    "                if keypoints is not None and len(keypoints) > 0:\n",
    "                    # Get preprocessing metadata\n",
    "                    _, img_metas = self.vitpose_wrapper.preprocess_frame(frame)\n",
    "                    if img_metas is None:\n",
    "                        return\n",
    "                        \n",
    "                    meta = img_metas[0]\n",
    "                    scale_factor = meta['scale_factor']  # [width_scale, height_scale]\n",
    "\n",
    "                    # Scale keypoints back to original frame size\n",
    "                    original_keypoints = []\n",
    "                    for kp in keypoints:\n",
    "                        orig_x = int(kp[0] * scale_factor[0])  # Scale x coordinate\n",
    "                        orig_y = int(kp[1] * scale_factor[1])  # Scale y coordinate\n",
    "                        original_keypoints.append((orig_x, orig_y, kp[2]))  # Keep confidence score\n",
    "\n",
    "                    # Draw skeleton\n",
    "                    confidence_threshold = 0.3\n",
    "                    for connection in self.vitpose_wrapper.keypoint_connections:\n",
    "                        pt1_idx, pt2_idx = connection\n",
    "                        if (original_keypoints[pt1_idx][2] > confidence_threshold and\n",
    "                                original_keypoints[pt2_idx][2] > confidence_threshold):\n",
    "                            pt1 = tuple(map(int, original_keypoints[pt1_idx][:2]))\n",
    "                            pt2 = tuple(map(int, original_keypoints[pt2_idx][:2]))\n",
    "                            color = (255, 255, 255) if not self.view_states['vitpose']['show_background'] else (0, 255, 0)\n",
    "                            cv2.line(display_frame, pt1, pt2, color, 2)\n",
    "\n",
    "                    # Draw keypoints\n",
    "                    for x, y, conf in original_keypoints:\n",
    "                        if conf > confidence_threshold:\n",
    "                            color = (255, 255, 255) if not self.view_states['vitpose']['show_background'] else (255, 0, 0)\n",
    "                            cv2.circle(display_frame, (int(x), int(y)), 4, color, -1)\n",
    "\n",
    "                # Convert frame for display\n",
    "                photo = self.prepare_photo(cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB))\n",
    "                if photo:\n",
    "                    self.vitpose_canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "                    self.vitpose_canvas.photo = photo\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating ViTPose display: {e}\")\n",
    "    # def update_wham_display(self, frame):\n",
    "    #     \"\"\"Update WHAM output display with toggle support\"\"\"\n",
    "    #     try:\n",
    "    #         if not self.view_states['wham']['show_background']:\n",
    "    #             display_frame = np.zeros(frame.shape, dtype=np.uint8)\n",
    "    #         else:\n",
    "    #             display_frame = frame.copy()\n",
    "                \n",
    "    #         # Add WHAM processing here\n",
    "    #         # Similar to MediaPipe, draw skeleton/mesh on either blank or original background\n",
    "            \n",
    "    #         photo = self.prepare_photo(cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB))\n",
    "    #         if photo:\n",
    "    #             self.wham_canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "    #             self.wham_canvas.photo = photo\n",
    "                \n",
    "    #     except Exception as e:\n",
    "    #         logging.error(f\"Error updating WHAM display: {e}\")\n",
    "\n",
    "    def update_mediapipe_display(self, frame):\n",
    "        \"\"\"Update MediaPipe output display with toggle support\"\"\"\n",
    "        try:\n",
    "            if self.mediapipe_pose:\n",
    "                # Create display frame based on toggle state\n",
    "                if not self.view_states['mediapipe']['show_background']:\n",
    "                    display_frame = np.zeros(frame.shape, dtype=np.uint8)\n",
    "                else:\n",
    "                    display_frame = frame.copy()\n",
    "\n",
    "                # Process with MediaPipe\n",
    "                results = self.mediapipe_pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                if results.pose_landmarks:\n",
    "                    if not self.view_states['mediapipe']['show_background']:\n",
    "                        # White skeleton on black background\n",
    "                        self.mp_drawing.draw_landmarks(\n",
    "                            display_frame,\n",
    "                            results.pose_landmarks,\n",
    "                            self.mp_pose.POSE_CONNECTIONS,\n",
    "                            landmark_drawing_spec=self.mp_drawing.DrawingSpec(\n",
    "                                color=(255, 255, 255),\n",
    "                                thickness=2,\n",
    "                                circle_radius=2\n",
    "                            ),\n",
    "                            connection_drawing_spec=self.mp_drawing.DrawingSpec(\n",
    "                                color=(255, 255, 255),\n",
    "                                thickness=2\n",
    "                            )\n",
    "                        )\n",
    "                    else:\n",
    "                        # Colored skeleton on video\n",
    "                        self.mp_drawing.draw_landmarks(\n",
    "                            display_frame,\n",
    "                            results.pose_landmarks,\n",
    "                            self.mp_pose.POSE_CONNECTIONS\n",
    "                        )\n",
    "                \n",
    "                # Convert and display the processed frame\n",
    "                photo = self.prepare_photo(cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB))\n",
    "                if photo:\n",
    "                    self.mediapipe_canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "                    self.mediapipe_canvas.photo = photo\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating MediaPipe display: {e}\")\n",
    "\n",
    "    def update_fourdhuman_display(self, frame):\n",
    "        \"\"\"Update 4DHuman output display with toggle support\"\"\"\n",
    "        try:\n",
    "            if not self.view_states['4dhuman']['show_background']:\n",
    "                display_frame = np.zeros(frame.shape, dtype=np.uint8)\n",
    "            else:\n",
    "                display_frame = frame.copy()\n",
    "            display_frame = self.FDHmodel.get_display_frame(frame)\n",
    "\n",
    "            photo = self.prepare_photo(cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB))\n",
    "            if photo:\n",
    "                self.fourdhuman_canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "                self.fourdhuman_canvas.photo = photo\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating 4DHuman display: {e}\")\n",
    "            \n",
    "    def play(self):\n",
    "        \"\"\"Play video with error handling\"\"\"\n",
    "        try:\n",
    "            if self.playing and self.cap is not None:\n",
    "                self.update_frame()\n",
    "                self.update_id = self.root.after(self.update_interval, self.play)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during playback: {e}\")\n",
    "            self.playing = False\n",
    "            \n",
    "    def prev_frame(self):\n",
    "        \"\"\"Go to previous frame\"\"\"\n",
    "        if self.cap is None:\n",
    "            return\n",
    "        self.current_frame = max(0, self.current_frame - 1)\n",
    "        self.update_frame()\n",
    "        \n",
    "    def next_frame(self):\n",
    "        \"\"\"Go to next frame\"\"\"\n",
    "        if self.cap is None:\n",
    "            return\n",
    "        self.current_frame = min(self.total_frames - 1, self.current_frame + 1)\n",
    "        self.update_frame()\n",
    "        \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        try:\n",
    "            if self.cap is not None:\n",
    "                self.cap.release()\n",
    "            if self.update_id:\n",
    "                self.root.after_cancel(self.update_id)\n",
    "            self.root.quit()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during cleanup: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitpose_weights_path = './ViTPose/checkpoints/vitpose_checkpoint.pth'\n",
    "vitpose_config_path = 'ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitPose+_base_coco+aic+mpii+ap10k+apt36k+wholebody_256x192_udp.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = PoseEstimationUI(root, vitpose_weights_path=vitpose_weights_path, vitpose_config_path = vitpose_config_path)\n",
    "        root.protocol(\"WM_DELETE_WINDOW\", app.cleanup)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PoseCompare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
