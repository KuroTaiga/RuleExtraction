{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3022667317.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git clone https://github.com/open-mmlab/mmcv.git\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/open-mmlab/mmcv.git\n",
    "cd mmcv\n",
    "git checkout v1.3.9\n",
    "MMCV_WITH_OPS=1 pip install -e .\n",
    "cd ..\n",
    "git clone https://github.com/ViTAE-Transformer/ViTPose.git\n",
    "cd ViTPose\n",
    "pip install -v -e .\n",
    "pip install timm==0.4.9 einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "class VitPoseWrapper:\n",
    "    \"\"\"Wrapper class for VitPose model initialization and inference\"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.load_model()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load VitPose model with pretrained weights\"\"\"\n",
    "        # Import here to avoid dependency issues if not using VitPose\n",
    "        from vitpose.models.model import ViTPose\n",
    "        \n",
    "        # Model configuration\n",
    "        model_cfg = {\n",
    "            'backbone': {\n",
    "                'type': 'ViT',\n",
    "                'img_size': [256, 192],\n",
    "                'patch_size': 16,\n",
    "                'embed_dim': 768,\n",
    "                'depth': 12,\n",
    "                'num_heads': 12,\n",
    "                'ratio': 1,\n",
    "                'use_checkpoint': False,\n",
    "                'mlp_ratio': 4,\n",
    "                'qkv_bias': True,\n",
    "                'drop_path_rate': 0.3,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ViTPose(model_cfg)\n",
    "        \n",
    "        # Load pretrained weights\n",
    "        weights_path = './ViTPose/weights/vitpose_large_coco_aic_mpii.pth'  # Update with actual path\n",
    "        state_dict = torch.load(weights_path, map_location=self.device)\n",
    "        \n",
    "        # Handle potential state dict differences\n",
    "        if 'state_dict' in state_dict:\n",
    "            state_dict = state_dict['state_dict']\n",
    "        \n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                k = k[7:]  # Remove 'module.' prefix\n",
    "            new_state_dict[k] = v\n",
    "            \n",
    "        model.load_state_dict(new_state_dict)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def preprocess_image(self, frame):\n",
    "        \"\"\"Preprocess image for VitPose model\"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize to model input size\n",
    "        frame_resized = cv2.resize(frame_rgb, (192, 256))\n",
    "        \n",
    "        # Apply normalization and convert to tensor\n",
    "        frame_tensor = self.transform(frame_resized)\n",
    "        frame_tensor = frame_tensor.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        return frame_tensor\n",
    "    \n",
    "    def postprocess_keypoints(self, heatmaps, original_shape):\n",
    "        \"\"\"Convert heatmaps to keypoint coordinates\"\"\"\n",
    "        # Get dimensions\n",
    "        height, width = original_shape[:2]\n",
    "        heatmap_height, heatmap_width = heatmaps.shape[2:]\n",
    "        \n",
    "        # Find keypoint locations from heatmaps\n",
    "        keypoints = []\n",
    "        confidences = []\n",
    "        \n",
    "        for heatmap in heatmaps[0]:\n",
    "            # Find the location of maximum activation\n",
    "            flat_id = torch.argmax(heatmap).item()\n",
    "            y = flat_id // heatmap_width\n",
    "            x = flat_id % heatmap_width\n",
    "            \n",
    "            # Get confidence score\n",
    "            confidence = heatmap[y, x].item()\n",
    "            \n",
    "            # Convert to original image coordinates\n",
    "            x_coord = int((x / heatmap_width) * width)\n",
    "            y_coord = int((y / heatmap_height) * height)\n",
    "            \n",
    "            keypoints.append((x_coord, y_coord))\n",
    "            confidences.append(confidence)\n",
    "            \n",
    "        return np.array(keypoints), np.array(confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseComparisonDebugger:\n",
    "    def __init__(self, video_path):\n",
    "        \"\"\"Initialize pose estimation models and video source\"\"\"\n",
    "        self.video_path = video_path\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # Initialize MediaPipe\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mediapipe_pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # Initialize VitPose\n",
    "        self.vitpose = VitPoseWrapper()\n",
    "        \n",
    "        # Initialize 4DHumans (placeholder)\n",
    "        self.humans4d_model = self.load_4dhumans_model()\n",
    "        \n",
    "        # Initialize WHAM (placeholder)\n",
    "        self.wham_model = self.load_wham_model()\n",
    "        \n",
    "        # Setup display\n",
    "        self.fig, self.axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        self.fig.suptitle('Pose Estimation Model Comparison')\n",
    "        \n",
    "        # Define keypoint connections for visualization\n",
    "        self.vitpose_connections = [\n",
    "            (15, 13), (13, 11), (16, 14), (14, 12),  # limbs\n",
    "            (11, 12), (5, 11), (6, 12),  # hip\n",
    "            (5, 6), (5, 7), (6, 8), (7, 9), (8, 10),  # spine and neck\n",
    "            (1, 2), (0, 1), (0, 2), (1, 3), (2, 4), (3, 5), (4, 6)  # face and shoulders\n",
    "        ]\n",
    "        \n",
    "    def process_frame_vitpose(self, frame):\n",
    "        \"\"\"Process frame with VitPose\"\"\"\n",
    "        # Preprocess frame\n",
    "        frame_tensor = self.vitpose.preprocess_image(frame)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            heatmaps = self.vitpose.model(frame_tensor)\n",
    "        \n",
    "        # Post-process to get keypoints\n",
    "        keypoints, confidences = self.vitpose.postprocess_keypoints(heatmaps, frame.shape)\n",
    "        \n",
    "        # Draw keypoints and connections\n",
    "        frame_out = frame.copy()\n",
    "        \n",
    "        # Draw connections\n",
    "        for connection in self.vitpose_connections:\n",
    "            if confidences[connection[0]] > 0.3 and confidences[connection[1]] > 0.3:\n",
    "                pt1 = tuple(map(int, keypoints[connection[0]]))\n",
    "                pt2 = tuple(map(int, keypoints[connection[1]]))\n",
    "                cv2.line(frame_out, pt1, pt2, (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw keypoints\n",
    "        for i, (x, y) in enumerate(keypoints):\n",
    "            if confidences[i] > 0.3:\n",
    "                cv2.circle(frame_out, (int(x), int(y)), 4, (255, 0, 0), -1)\n",
    "        \n",
    "        return frame_out\n",
    "    \n",
    "    # ... [rest of the PoseComparisonDebugger class remains the same]\n",
    "\n",
    "# Example usage\n",
    "def debug_pose_estimation(video_path):\n",
    "    \"\"\"\n",
    "    Create and run the pose estimation debugger\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file to analyze\n",
    "    \"\"\"\n",
    "    debugger = PoseComparisonDebugger(video_path)\n",
    "    debugger.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
